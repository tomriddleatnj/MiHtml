import sqlite3
import google.generativeai as genai
import json
import time
import random
import re
import os
from concurrent.futures import ThreadPoolExecutor, as_completed

# ================= é…ç½®åŒºåŸŸ =================
# â˜…â˜…â˜… Paid Tier API Key Config â˜…â˜…â˜…
API_KEY = "AIzaSyDZgWK8dJr13C9SdFtAq1Hm_9YOgI1edZI"

# Paid Tier Optimization Config
BATCH_SIZE = 50           # æ¯ä¸ª API è¯·æ±‚åŒ…å«çš„å•è¯æ•°
MAX_WORKERS = 20           # å¹¶å‘çº¿ç¨‹æ•°
SUPER_BATCH_SIZE = BATCH_SIZE * MAX_WORKERS 

DB_NAME = "vocab_project.db"
SOURCE_FILE = "wordsdata_es.txt"
DEFAULT_MODEL = "gemini-2.5-flash" 

TAG_LIST_STR = """
[Professional Tags]
office, hr, finance, legal, it, ops, marketing, bd, procurement, qhse,
pm, bidding, supervision, water, transport, rail, roads, airport, ports, energy, urban, geo, environment

[General Categories]
comm (communication), abstract (logic/time/numbers), society (politics/history)
"""

if API_KEY == "gen-lang-client-0577078086":
    print("âš ï¸ è­¦å‘Š: è¯·åœ¨ vocab_worker.py ä¸­é…ç½®æ­£ç¡®çš„ API_KEY")
else:
    genai.configure(api_key=API_KEY)

# ================= è¾…åŠ©å‡½æ•° =================

def clean_json_string(text):
    text = re.sub(r'^```json\s*', '', text, flags=re.MULTILINE)
    text = re.sub(r'^```\s*', '', text, flags=re.MULTILINE)
    text = re.sub(r'\s*```$', '', text, flags=re.MULTILINE)
    return text.strip()

def init_db():
    conn = sqlite3.connect(DB_NAME)
    conn.execute('PRAGMA journal_mode=WAL;') 
    cursor = conn.cursor()
    
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS vocab_staging (
            word TEXT PRIMARY KEY,
            level TEXT,
            hint TEXT,
            tags TEXT,
            status TEXT,
            processed_flag INTEGER DEFAULT 0,
            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    ''')
    
    existing_cols = [row[1] for row in cursor.execute("PRAGMA table_info(vocab_staging)")]
    new_columns = {
        "definition_cn": "TEXT",
        "phonetic": "TEXT",
        "context": "TEXT",
        "translated_flag": "INTEGER DEFAULT 0" 
    }
    for col_name, col_type in new_columns.items():
        if col_name not in existing_cols:
            print(f"ğŸ”§ å‡çº§æ•°æ®åº“: æ·»åŠ  {col_name}...")
            cursor.execute(f"ALTER TABLE vocab_staging ADD COLUMN {col_name} {col_type}")

    cursor.execute('''CREATE TABLE IF NOT EXISTS word_slots (slot_id INTEGER, word TEXT, filename TEXT, exported_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP, PRIMARY KEY (slot_id, word))''')
    cursor.execute('''CREATE TABLE IF NOT EXISTS app_config (key TEXT PRIMARY KEY, value TEXT)''')
    
    # é»˜è®¤é…ç½®
    cursor.execute("INSERT OR IGNORE INTO app_config (key, value) VALUES (?, ?)", ('model_name', DEFAULT_MODEL))
    # â˜…â˜…â˜… æ–°å¢: é»˜è®¤çŠ¶æ€ä¸º pausedï¼Œé˜²æ­¢å¯åŠ¨å³æš´èµ° â˜…â˜…â˜…
    cursor.execute("INSERT OR IGNORE INTO app_config (key, value) VALUES (?, ?)", ('worker_status', 'paused'))
    
    conn.commit()
    return conn

def get_config_value(conn, key, default=None):
    try:
        cursor = conn.cursor()
        cursor.execute("SELECT value FROM app_config WHERE key=?", (key,))
        row = cursor.fetchone()
        return row[0] if row else default
    except:
        return default

def load_data_to_db(conn):
    cursor = conn.cursor()
    cursor.execute("SELECT count(*) FROM vocab_staging")
    if cursor.fetchone()[0] > 0: return

    if not os.path.exists(SOURCE_FILE):
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æºæ–‡ä»¶ {SOURCE_FILE}")
        return

    print("ğŸ“¥ æ­£åœ¨å¯¼å…¥åŸå§‹æ•°æ®...")
    with open(SOURCE_FILE, 'r', encoding='utf-8') as f:
        to_insert = []
        for line in f:
            parts = line.strip().split('\t')
            if len(parts) < 3: continue
            word, level, hint = parts[0].strip(), parts[-2].strip(), parts[-1].strip()
            to_insert.append((word, level, hint, "[]", "pending", 0))
    
    cursor.executemany('INSERT OR IGNORE INTO vocab_staging (word, level, hint, tags, status, processed_flag) VALUES (?, ?, ?, ?, ?, ?)', to_insert)
    conn.commit()
    print(f"âœ… æˆåŠŸå¯¼å…¥ {len(to_insert)} æ¡æ•°æ®ã€‚")

# ================= AI ä»»åŠ¡é€»è¾‘ (å¹¶å‘ç‰ˆ) =================

def call_ai_with_retry(model, prompt, model_name, task_type):
    retries = 3
    base_wait = 1
    
    for attempt in range(retries):
        try:
            response = model.generate_content(prompt)
            cleaned = clean_json_string(response.text)
            return json.loads(cleaned)
        except Exception as e:
            err_msg = str(e)
            print(f"  âš ï¸ {task_type} Error ({model_name} - {attempt+1}/{retries}): {err_msg}")
            
            if "429" in err_msg:
                wait_match = re.search(r'retry in (\d+(\.\d+)?)s', err_msg)
                wait_time = float(wait_match.group(1)) + 1 if wait_match else 5
                print(f"  ğŸ›‘ Rate Limit (429). Thread sleeping {wait_time:.1f}s...")
                time.sleep(wait_time)
                continue
            
            time.sleep(base_wait)
            
    return None 

def process_classify_chunk(chunk_data, model_name):
    model = genai.GenerativeModel(model_name, generation_config={"response_mime_type": "application/json"})
    input_list = [f"{w} (Def: {h})" for w, l, h in chunk_data]
    
    prompt = f"""
    Role: Spanish linguistic expert.
    Tags: {TAG_LIST_STR}
    Task: Classify words. Return empty tags [] if no fit.
    Input: {json.dumps(input_list)}
    Output JSON: [{{"word": "word1", "tags": ["tag1"]}}]
    """
    
    result = call_ai_with_retry(model, prompt, model_name, "Classify")
    return chunk_data, result

def process_translate_chunk(chunk_data, model_name):
    model = genai.GenerativeModel(model_name, generation_config={"response_mime_type": "application/json"})
    input_list = [{"word": w, "hint": h} for w, _, h in chunk_data]
    
    prompt = f"""
    Role: Expert Spanish-Chinese Translator.
    Task: Provide Chinese definition, IPA phonetic, and a simple Spanish context sentence for each word.
    Input: {json.dumps(input_list)}
    Output JSON Format:
    [
      {{"word": "ordenador", "definition": "ç”µè„‘", "phonetic": "/oÉ¾.Ã°e.naËˆÃ°oÉ¾/", "context": "Mi ordenador es nuevo."}}
    ]
    """
    
    result = call_ai_with_retry(model, prompt, model_name, "Translate")
    return chunk_data, result

# ================= ä¸»ç¨‹åº =================

def main():
    conn = init_db()
    load_data_to_db(conn)
    cursor = conn.cursor()
    
    print(f"ğŸš€ Worker å¯åŠ¨ (Parallel Mode) | Threads: {MAX_WORKERS}")
    
    executor = ThreadPoolExecutor(max_workers=MAX_WORKERS)
    
    # çŠ¶æ€æ‰“å°å»é‡
    last_status_print = ""

    while True:
        # â˜…â˜…â˜… æ§åˆ¶é€»è¾‘æ ¸å¿ƒ â˜…â˜…â˜…
        # 1. è·å–æœ€æ–°é…ç½®
        current_model = get_config_value(conn, 'model_name', DEFAULT_MODEL)
        worker_status = get_config_value(conn, 'worker_status', 'paused')

        # 2. å¦‚æœæš‚åœï¼Œåˆ™ç©ºè½¬
        if worker_status != 'running':
            if last_status_print != "paused":
                print(f"â¸ï¸ Worker å·²æš‚åœ (Status: {worker_status}). ç­‰å¾…æŒ‡ä»¤...")
                last_status_print = "paused"
            time.sleep(2)
            continue
        
        if last_status_print != "running":
            print(f"â–¶ï¸ Worker è¿è¡Œä¸­ (Model: {current_model})...")
            last_status_print = "running"

        # --- 3. Classify Logic ---
        cursor.execute("SELECT word, level, hint FROM vocab_staging WHERE processed_flag = 0 LIMIT ?", (SUPER_BATCH_SIZE,))
        super_batch = cursor.fetchall()
        
        if super_batch:
            print(f"\nğŸ”„ [Classify] Processing {len(super_batch)} words...")
            chunks = [super_batch[i:i + BATCH_SIZE] for i in range(0, len(super_batch), BATCH_SIZE)]
            
            futures = []
            for chunk in chunks:
                futures.append(executor.submit(process_classify_chunk, chunk, current_model))
            
            db_updates = []
            db_errors = []
            
            for future in as_completed(futures):
                original_chunk, res_json = future.result()
                if res_json is None:
                    for w, l, h in original_chunk: db_errors.append((w,))
                else:
                    res_map = {i['word']: i.get('tags', []) for i in res_json if 'word' in i}
                    for w, l, h in original_chunk:
                        tags = res_map.get(w, [])
                        status = 'keep' if tags or l in ['A1', 'A2', 'B1'] else 'discard'
                        final_tags = tags if tags else (['basic'] if status=='keep' else [])
                        db_updates.append((json.dumps(final_tags), status, 1, w))

            if db_updates:
                cursor.executemany("UPDATE vocab_staging SET tags=?, status=?, processed_flag=?, updated_at=CURRENT_TIMESTAMP WHERE word=?", db_updates)
                print(f"  âœ… Saved {len(db_updates)} classifications.")
            if db_errors:
                cursor.executemany("UPDATE vocab_staging SET processed_flag=2, updated_at=CURRENT_TIMESTAMP WHERE word=?", db_errors)
            
            conn.commit()
            continue 

        # --- 4. Translate Logic ---
        cursor.execute("SELECT word, level, hint FROM vocab_staging WHERE status='keep' AND translated_flag=0 LIMIT ?", (SUPER_BATCH_SIZE,))
        super_batch_trans = cursor.fetchall()
        
        if super_batch_trans:
            print(f"\nğŸ”¤ [Translate] Processing {len(super_batch_trans)} words...")
            chunks = [super_batch_trans[i:i + BATCH_SIZE] for i in range(0, len(super_batch_trans), BATCH_SIZE)]
            
            futures = []
            for chunk in chunks:
                futures.append(executor.submit(process_translate_chunk, chunk, current_model))
            
            db_updates = []
            db_errors = []
            
            for future in as_completed(futures):
                original_chunk, res_json = future.result()
                if res_json is None:
                    for w, l, h in original_chunk: db_errors.append((w,))
                else:
                    res_map = {i['word']: i for i in res_json if 'word' in i}
                    for w, l, h in original_chunk:
                        info = res_map.get(w, {})
                        db_updates.append((info.get('definition', ''), info.get('phonetic', ''), info.get('context', ''), 1, w))

            if db_updates:
                cursor.executemany("UPDATE vocab_staging SET definition_cn=?, phonetic=?, context=?, translated_flag=?, updated_at=CURRENT_TIMESTAMP WHERE word=?", db_updates)
                print(f"  âœ… Saved {len(db_updates)} translations.")
            if db_errors:
                cursor.executemany("UPDATE vocab_staging SET translated_flag=2, updated_at=CURRENT_TIMESTAMP WHERE word=?", db_errors)
                
            conn.commit()
            continue

        print("ğŸ’¤ Queue Empty. Waiting 5s...")
        time.sleep(5)

if __name__ == "__main__":
    main()